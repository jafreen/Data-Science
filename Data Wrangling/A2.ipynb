{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9Dv6ZxEpxZN"
   },
   "source": [
    "# Tokenozation and stop-word removal",
    "\n",
    "Date: 04/06/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used, e.g.,:\n",
    "* pandas \n",
    "* re \n",
    "* numpy\n",
    "* nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzrU77l0pxZP"
   },
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sO8PvpkHpxZQ"
   },
   "outputs": [],
   "source": [
    "# Code to import libraries, e.g.,\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import *\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sA69D-E9pxZX"
   },
   "source": [
    "## 2. Loading the File for Part A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BjBXTY4upxZX"
   },
   "outputs": [],
   "source": [
    "small_file = './smallFileNoURL.txt'\n",
    "big_file = './bigFile.txt'\n",
    "stopword_file = './stopwords_en.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now tokenizing the file using regular expression - r\"\\w+(?:[-.]\\w+)?\n",
    "def tokenize_desc(raw_text):\n",
    "    from nltk.tokenize import RegexpTokenizer \n",
    "    raw_text = raw_text.lower()\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
    "    unigram_tokens = tokenizer.tokenize(raw_text)\n",
    "    \n",
    "    return [word for word in unigram_tokens if word.isalpha()]\n",
    "    \n",
    "    #return unigram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now concatenating the word again \n",
    "def word_concat(dsd):\n",
    "    \"\"\"\n",
    "    concatenate all the words stored in the values of a given dictionary. Each value is a list\n",
    "    of tokenized sentences.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for key in dsd:\n",
    "        #print (key)\n",
    "        all_words += dsd[key]\n",
    "    #print (\"tokens:\", len(all_words))\n",
    "    #print (\"types:\", len(set(all_words)))\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now removing stop-words\n",
    "\n",
    "def remove_words(words, stops):\n",
    "    \"\"\"\n",
    "    This function excludes all the words appearing in a given list.\n",
    "    Here the list is named \"stops\"\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will remove the urls \n",
    "def remove_urls(words):\n",
    "    tokenizer = RegexpTokenizer(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''')\n",
    "    return [word for word in words if word not in tokenizer.tokenize(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This fudtion will remove the emails\n",
    "\n",
    "def remove_emails(words):\n",
    "    \n",
    "    #tokenizer = RegexpTokenizer(r\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\")\n",
    "    tokenizer = RegexpTokenizer(r\"(\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,6})\")\n",
    "    #tokenizer = RegexpTokenizer(r\"^[A-Za-z0-9\\.\\+_-]+@[A-Za-z0-9\\._-]+\\.[a-zA-Z]+$\")\n",
    "    \n",
    "    return [word for word in words if word not in tokenizer.tokenize(word)]                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function creation for both PART A and PART B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(small_file, small_or_big):\n",
    "    file_reader = open(small_file) ## reading the file\n",
    "    sents = []\n",
    "    for line in file_reader.readlines():\n",
    "        line = line.strip();\n",
    "        if line != '': \n",
    "            sents.append(line)\n",
    "    job_dict = {}\n",
    "    j=0\n",
    "    for sent in sents:\n",
    "        j=j+1\n",
    "        if j==1:\n",
    "            tokenizer = RegexpTokenizer(r\"\\#\\d*\")\n",
    "            idstring=tokenizer.tokenize(sent)        \n",
    "        if j==3:\n",
    "            job_dict[str(idstring)]=sent\n",
    "            j=0\n",
    "\n",
    "    for key in job_dict:\n",
    "        job_dict[key] = tokenize_desc(job_dict[key])\n",
    "\n",
    "    all_words=word_concat(job_dict)\n",
    "\n",
    "    stopwords = []\n",
    "    with open('./stopwords_en.txt') as f: ## stopword removal\n",
    "        stopwords = f.read().splitlines()\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    all_words_wo_sw=remove_words(all_words, stopwords)\n",
    "    all_words_wo_sw=remove_urls(all_words_wo_sw)\n",
    "    all_words_wo_sw=remove_emails(all_words_wo_sw)\n",
    "\n",
    "    freq_dist = FreqDist(all_words_wo_sw)\n",
    "\n",
    "    #GT100Words = [str(freq)+\":\"+str(word) for word, freq in freq_dist.items() if freq > 100 ] ## finding the high frequency word which occured more than 100 times\n",
    "    GT100Words = []\n",
    "    for word, freq in freq_dist.items():\n",
    "        if freq >100:\n",
    "            GT100Words.append([word,freq])\n",
    "    \n",
    "    #print(GT100Words)\n",
    "    #GT100Words.sort(reverse=True)\n",
    "    GT100Words=sorted(GT100Words, key=lambda x: x[1], reverse=True)\n",
    "    GT100Words = GT100Words[0:100]\n",
    "    #print(GT100Words)\n",
    "    v_writer = open(small_or_big+\"FileNoURL_highFreq.txt\", \"w\")\n",
    "\n",
    "    for values in GT100Words:\n",
    "        v_writer.write(values[0]+\":\"+str(values[1])+\"\\n\")\n",
    "        #v_writer.write(key+\":\"+value+\"\\n\")\n",
    "    v_writer.close()\n",
    "    \n",
    "    WordsInOneAdvertOnly = freq_dist.hapaxes()\n",
    "\n",
    "    v_writer = open(small_or_big+\"FileNoURL_lowFreq.txt\", \"w\")\n",
    "\n",
    "    for value in WordsInOneAdvertOnly:\n",
    "        v_writer.write(value+\"\\n\")\n",
    "    v_writer.close()\n",
    "    \n",
    "    vocab=set(all_words_wo_sw)\n",
    "    vocab_sorted = list(sorted(vocab))\n",
    "\n",
    "    v_writer = open(small_or_big+\"FileNoURL_vocab.txt\", \"w\")\n",
    "    i=0\n",
    "    for type in vocab_sorted:\n",
    "        i=i+1\n",
    "        v_writer.write(type+\":\"+str(i)+\"\\n\")\n",
    "    v_writer.close()\n",
    "    \n",
    "    final_jobs ={}\n",
    "    for key, value in job_dict.items():\n",
    "        #print(value)\n",
    "        final_jobs[key] = remove_words(value, stopwords)   \n",
    "\n",
    "    w_writer = open(small_or_big+\"FileNoURL_sparse.txt\", \"w\")\n",
    "    for doc, sents in final_jobs.items():      \n",
    "        w_writer.write(doc)\n",
    "        fd = FreqDist(sents)\n",
    "        ## creating the vocabulary list\n",
    "        for word, count in fd.items():\n",
    "            w_writer.write(\",{0}:{1}\".format(vocab_sorted.index(word)+1, count))\n",
    "        w_writer.write(\"\\n\")\n",
    "    w_writer.close()\n",
    "    \n",
    "    ## printing bigrams words\n",
    "    bigrams = ngrams(all_words_wo_sw, n = 2)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "        \n",
    "    v_writer = open(small_or_big+\"FileNoURL_ngram.txt\", \"w\")\n",
    "\n",
    "    for value in fdbigram.most_common():\n",
    "        v_writer.write(str(value)+\"\\n\")\n",
    "    \n",
    "    ## prnting trigram words\n",
    "    bigrams = ngrams(all_words_wo_sw, n = 3)\n",
    "    fdbigram = FreqDist(bigrams)\n",
    "\n",
    "    for value in fdbigram.most_common():\n",
    "        v_writer.write(str(value)+\"\\n\")\n",
    "\n",
    "    w_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file(small_file, \"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
  Part B in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file(big_file, \"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5D4o0tXGhwiM"
   },
   "source": [
    "# 3. Time My Code\n",
    "This is how we time a cell of code. You should put **all codes** for  \"Part B: Big file with URLs\" in one cell unless the definitions of the functions you defined in Part A and use in both parts (perhaps with different paramter settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7151,
     },
     "user_tz": -600
    },
    "id": "FDQhOCqQpxZc",
    "outputId": "966cf1f7-bee4-4108-f441-fa480e25cf09"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10000):\n",
    "    j = i**i\n",
    "    if i % 1000 ==1:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EcdPOIfSpxZe"
   },
   "source": [
    "## 3. Summary\n",
    "Initially the files were loaded. The trasaction ID and description were extracted from the file. Then tokenization and stopword removal was done. Then for both Part A and B a function was created which can create:\n",
    "1. The first text file lists the vocabulary of the unique word and their index\n",
    "2. The second text file lists the adertisement ID, word index and frequency of words under the respective advertisement ID\n",
    "3. The second text file shows the unigram which only apears once in one of the description file\n",
    "4. The third text file shows the 100 most frequent unigrams and their frequency count from high to low\n",
    "5. The fifth file shows the list of bigrams and trigrams and their frequency \n",
    "\n",
    "The function also removes URLS and emails from the big file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsoh2-U_qiBo"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "A2_new.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
