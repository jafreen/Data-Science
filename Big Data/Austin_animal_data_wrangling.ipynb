{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crbocKr_HqDW"
   },
   "source": [
    "### Group 4 Data Wrangling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier Armitage 29404290\n",
    "\n",
    "Jafreen Hossain 29398711\n",
    "\n",
    "Efendy Kurniawan 29958830\n",
    "\n",
    "Jian Liang 30137713\n",
    "\n",
    "Alannah Page 25117114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1w2YRhYMHqDX"
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to7hOTgFHqDY",
    "outputId": "cb77a7f3-870b-4a03-8387-9921b1888893",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://990ab9a36641:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1560165462832)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import java.util.Calendar\n",
       "import java.io\n",
       "import java.text.SimpleDateFormat\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.ml.clustering.LDA\n",
       "import org.apache.spark.mllib.linalg.{VectorUDT, Vectors}\n",
       "import org.apache.spark.sql.{Row, SQLContext}\n",
       "import org.apache.spark.sql.types.{StructField, StructType}\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.{OneHotEncoderEstimator, StringIndexer, StringIndexerModel}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import java.util.Calendar\n",
    "import java.io\n",
    "import java.text.SimpleDateFormat\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.ml.clustering.LDA\n",
    "import org.apache.spark.mllib.linalg.{VectorUDT, Vectors}\n",
    "import org.apache.spark.sql.{Row, SQLContext}\n",
    "import org.apache.spark.sql.types.{StructField, StructType}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.{OneHotEncoderEstimator, StringIndexer, StringIndexerModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TwhqI5OHqDe"
   },
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k2qs5AmHqDe"
   },
   "source": [
    "##### Import and define schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFujvEDkHqDf",
    "outputId": "01bd81fa-cc7b-479f-9a5f-28054627faa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import_data_set_schema: (intake: String, outcome: String)(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_data_set_schema(intake: String, outcome: String): (DataFrame, DataFrame) = {\n",
    "\n",
    "    var in_df = spark.read.option(\"header\",\"true\").csv(intake)\n",
    "    var out_df = spark.read.option(\"header\",\"true\").csv(outcome)\n",
    "    val inCols = Seq(\"animal_id\",\"name_in\",\"datetime_in\",\"monthYear_in\",\"location_found\",\"intake_type\", \"intake_condition\",\"species_in\",\"sex_in\",\"age_in\",\"breed_in\",\"color_in\")\n",
    "    val outCols = Seq(\"animal_id\",\"name_out\",\"datetime_out\",\"monthYear_out\",\"dob\",\"outcome_type\",\"outcome_subtype\",\"species_out\",\"sex_out\",\"age_out\",\"breed_out\",\"color_out\")\n",
    "    in_df = in_df.toDF(inCols: _*)\n",
    "    out_df = out_df.toDF(outCols: _*)\n",
    "\n",
    "    return (in_df, out_df)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0g4AGOAUHqDi"
   },
   "source": [
    "##### Create Unique ID for each row based on animal_id and datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4aRcO2_HqDj",
    "outputId": "692022a7-0390-4406-ec7c-866b3693d517",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_unique_id: (df1: org.apache.spark.sql.DataFrame, df2: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_unique_id(df1: DataFrame, df2: DataFrame): (DataFrame, DataFrame) = {\n",
    "\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "\n",
    "    val df1_unique = spark.sql(\n",
    "    \"\"\"SELECT CONCAT(animal_id, '_', CAST(rownum AS STRING)) AS animal_id_unique\n",
    "            ,name_in\n",
    "            ,datetime_in\n",
    "            ,location_found\n",
    "            ,intake_type\n",
    "            ,intake_condition\n",
    "            ,species_in\n",
    "            ,sex_in\n",
    "            ,age_in\n",
    "            ,breed_in\n",
    "            ,color_in\n",
    "    FROM (\n",
    "        SELECT `animal_id`\n",
    "            ,`name_in`\n",
    "            ,`datetime_in`\n",
    "            ,`location_found`\n",
    "            ,`intake_type`\n",
    "            ,`intake_condition`\n",
    "            ,`species_in`\n",
    "            ,`sex_in`\n",
    "            ,`age_in`\n",
    "            ,`breed_in`\n",
    "            ,`color_in`\n",
    "            ,ROW_NUMBER() OVER (PARTITION BY `animal_id` ORDER BY `datetime_in` DESC) AS rownum\n",
    "        FROM df1\n",
    "    )\n",
    "    \"\"\")\n",
    "    df1_unique.createOrReplaceTempView(\"df1_unique\")\n",
    "\n",
    "    val df2_unique = spark.sql(\n",
    "    \"\"\"SELECT CONCAT(animal_id, '_', CAST(rownum AS STRING)) AS animal_id_unique\n",
    "            ,datetime_out\n",
    "            ,name_out\n",
    "            ,dob\n",
    "            ,outcome_type\n",
    "            ,outcome_subtype\n",
    "            ,species_out\n",
    "            ,sex_out\n",
    "            ,age_out\n",
    "            ,breed_out\n",
    "            ,color_out\n",
    "    FROM (\n",
    "        SELECT `animal_id`\n",
    "            ,`datetime_out`\n",
    "            ,`name_out`\n",
    "            ,`dob`\n",
    "            ,`outcome_type`\n",
    "            ,`outcome_subtype`\n",
    "            ,`species_out`\n",
    "            ,`sex_out`\n",
    "            ,`age_out`\n",
    "            ,`breed_out`\n",
    "            ,`color_out`\n",
    "            ,ROW_NUMBER() OVER (PARTITION BY `animal_id` ORDER BY `datetime_out` DESC) AS rownum\n",
    "        FROM df2 \n",
    "    )\n",
    "    \"\"\")\n",
    "    df2_unique.createOrReplaceTempView(\"df2_unique\")\n",
    "   \n",
    "    return (df1_unique, df2_unique)\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKpyy_aqHqDm"
   },
   "source": [
    "##### Convert age column into consistent format (years decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1MYM4_wHqDn",
    "outputId": "3c7f43d2-c58b-42e2-b82a-e3c9c133ab07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_age: (df: org.apache.spark.sql.DataFrame, sourceTable: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_age(df: DataFrame, sourceTable: String): DataFrame = {\n",
    "    // define function\n",
    "\tdef convert_age(input:String) : Float = {\n",
    "        if (input == \"NULL\") {\n",
    "            var age = -1.toFloat\n",
    "            return age\n",
    "        }\n",
    "\t    if (input.split(\"\\\\D\")(0).size > 0){\n",
    "\t        var num = input.split(\"\\\\D\")(0).toFloat\n",
    "\t        if (input.matches(\".*months?\")){\n",
    "\t            num = num / 12\n",
    "\t        } else if (input.matches(\".*weeks?\")) {\n",
    "\t            num = (num*7)/365\n",
    "\t        } else if (input.matches(\".*days?\")) {\n",
    "\t            num = num/365\n",
    "\t        }\n",
    "\t        return num\n",
    "\t    } else {\n",
    "\t        var age = -1.toFloat\n",
    "\t        return age\n",
    "\t    }\n",
    "\n",
    "\t}\n",
    "\n",
    "\t// Generate udf to apply to DataFrame column\n",
    "\timport org.apache.spark.sql.functions.udf\n",
    "\tval convertAge = udf[Float, String](convert_age)\n",
    "\n",
    "\t// Add new age column\n",
    "\tif (sourceTable == \"in\"){\n",
    "\t\tval df_updated = df.withColumn(\"age_years_in\", convertAge(scala.Symbol(\"age_in\")))\n",
    "\t\treturn df_updated\n",
    "\t} else if (sourceTable == \"out\") {\n",
    "\t\tval df_updated = df.withColumn(\"age_years_out\", convertAge(scala.Symbol(\"age_out\")))\n",
    "\t\treturn df_updated\n",
    "\t} else {\n",
    "\t\treturn df\n",
    "\t}\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFZlxGM-HqDp"
   },
   "source": [
    "##### Convert sex column into two separate columns including sex and a boolean variable for desexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLglbs2AHqDq",
    "outputId": "059d7f27-44d0-4548-fd0a-fe692178c1d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_sex: (df: org.apache.spark.sql.DataFrame, sourceTable: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_sex(df: DataFrame, sourceTable: String): DataFrame= {\n",
    "    def extract_sex(input:String) : String = {\n",
    "        var sex = \"Unknown\"\n",
    "        if (input.toLowerCase.matches(\"(.*[^(\\\\w)]male)|^male.*\")) {\n",
    "            sex = \"male\"\n",
    "        } else if (input.toLowerCase.matches(\"(.*[^(\\\\w)]female)|^female.*\")) {\n",
    "            sex = \"female\"\n",
    "        }\n",
    "        return sex\n",
    "    }\n",
    "\n",
    "    def extract_desexed(input:String) : String = {\n",
    "        var desexed = \"Unknown\"\n",
    "        if (input.toLowerCase.matches(\"(.*neutered.*)|(.*spayed.*)\")) {\n",
    "            desexed = \"true\"\n",
    "        } else if (input.toLowerCase contains \"intact\"){\n",
    "            desexed = \"false\"\n",
    "        }\n",
    "        return desexed\n",
    "    }\n",
    "\n",
    "    // Generate udf to apply to DataFrame column\n",
    "    import org.apache.spark.sql.functions.udf\n",
    "    val extractSex = udf[String, String](extract_sex)\n",
    "    val extractDesex = udf[String, String](extract_desexed)\n",
    "\n",
    "    // Add new sex and desexed columns\n",
    "    if (sourceTable == \"in\") {\n",
    "        var updated_df = df.withColumn(\"sex_only\", extractSex(scala.Symbol(\"sex_in\")))\n",
    "        updated_df = updated_df.withColumn(\"desexed_in\", extractDesex(scala.Symbol(\"sex_in\")))\n",
    "        return updated_df\n",
    "    } else if (sourceTable == \"out\") {\n",
    "        var updated_df = df.withColumn(\"sex_only\", extractSex(scala.Symbol(\"sex_out\")))\n",
    "        updated_df = updated_df.withColumn(\"desexed_out\", extractDesex(scala.Symbol(\"sex_out\")))\n",
    "        return updated_df\n",
    "    } else {\n",
    "        return df\n",
    "\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LweuPjIfHqDt"
   },
   "source": [
    "##### Convert colour column into primary and secondary colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v54GHI9vHqDu",
    "outputId": "5c928ea5-3a23-4d20-b972-24b97b65a6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_colour: (df: org.apache.spark.sql.DataFrame, sourceTable: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_colour(df: DataFrame, sourceTable: String): DataFrame = {\n",
    "\n",
    "    def extract_prim (input:String) : String = {\n",
    "        if (input.matches(\".*/.*\")){\n",
    "            var pri_colour = input.toLowerCase.split(\"/\")(0).split(\" \")(0)\n",
    "            return pri_colour\n",
    "        } else {\n",
    "            var pri_colour = input.toLowerCase.split(\" \")(0)\n",
    "            return pri_colour\n",
    "        }   \n",
    "    }\n",
    "\n",
    "    def extract_sec (input:String) : String = {\n",
    "        if (input.matches(\".*/.*\")){\n",
    "            var sec_colour = input.toLowerCase.split(\"/\")(1).split(\" \")(0)\n",
    "            return sec_colour\n",
    "        } else if (input.matches(\".+ .+\")) {\n",
    "            var sec_colour = input.toLowerCase.split(\" \")(1)\n",
    "            return sec_colour\n",
    "        }  else {\n",
    "            var sec_colour = null\n",
    "            return sec_colour\n",
    "        } \n",
    "    }\n",
    "\n",
    "    // Generate udf to apply to DataFrame column\n",
    "    val extractPrim = udf[String, String](extract_prim)\n",
    "    val extractSec = udf[String, String](extract_sec)\n",
    "    \n",
    "    // Add new colour cols\n",
    "\n",
    "    if (sourceTable == \"in\") {\n",
    "        var updated_df = df.withColumn(\"prim_colour_in\", extractPrim(scala.Symbol(\"color_in\")))\n",
    "        updated_df = updated_df.withColumn(\"sec_colour_in\", extractSec(scala.Symbol(\"color_in\")))\n",
    "        return updated_df\n",
    "    } else if (sourceTable == \"out\") {\n",
    "        var updated_df = df.withColumn(\"prim_colour_out\", extractPrim(scala.Symbol(\"color_out\")))\n",
    "        updated_df = updated_df.withColumn(\"sec_colour_out\", extractSec(scala.Symbol(\"color_out\")))\n",
    "        return updated_df\n",
    "    } else {\n",
    "        return df\n",
    "    }    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Byl3u4loHqDx"
   },
   "source": [
    "##### Convert breed column into primary and mixed breed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_breed: (df: org.apache.spark.sql.DataFrame, sourceType: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_breed(df: DataFrame, sourceType: String): DataFrame = {\n",
    "    \n",
    "    def extract_mix (input:String) : String = {\n",
    "        if (input.toLowerCase.matches(\".*mix.*\") || input.matches(\".*/.*\")){\n",
    "            var mix_bool = \"true\"\n",
    "            return mix_bool\n",
    "        } else {\n",
    "            var mix_bool = \"false\"\n",
    "            return mix_bool\n",
    "        }   \n",
    "    }\n",
    "    \n",
    "    def extract_prim_breed (input:String) : String = {\n",
    "        if (input.matches(\".*/.*\")){\n",
    "            var pri_breed = input.toLowerCase.split(\"/\")(0).replaceAll(\"mix\", \"\").trim\n",
    "            return pri_breed\n",
    "        } else {\n",
    "            var pri_breed = input.toLowerCase.replaceAll(\"mix\", \"\").trim\n",
    "            return pri_breed\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    \n",
    "    // Generate udf to apply to DataFrame column\n",
    "    val extractMix = udf[String, String](extract_mix)\n",
    "    val extractPrim = udf[String, String](extract_prim_breed)\n",
    "    \n",
    "    if (sourceType == \"in\") {\n",
    "        var updated_df = df.withColumn(\"mix_bool\", extractMix(scala.Symbol(\"breed_in\")))\n",
    "        updated_df = updated_df.withColumn(\"prim_breed_in\", extractPrim(scala.Symbol(\"breed_in\")))\n",
    "        return updated_df\n",
    "    } else if (sourceType == \"out\") {\n",
    "        var updated_df = df.withColumn(\"mix_bool_out\", extractMix(scala.Symbol(\"breed_out\")))\n",
    "        updated_df = df.withColumn(\"prim_breed_out\", extractPrim(scala.Symbol(\"breed_out\")))\n",
    "        return updated_df\n",
    "    } else {\n",
    "        return df\n",
    "    } \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqD12jXbHqD3"
   },
   "source": [
    "##### Datetime column wrangling into consistent format and generation of new date related columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2DcwR5THqD3",
    "outputId": "a1b2cdef-78aa-4464-af7d-29c219a80603"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date_Time_Wrangler: (tgtDF: org.apache.spark.sql.DataFrame, colName: String, sourceType: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "= Credit\n",
    "Created by : Jian Liang\n",
    "Last modified: 26th May 2019\n",
    "\n",
    "= Discription:\n",
    "This is function is to wranggle a Date/DateTime Column as following procedure.\n",
    "\n",
    "1. For \"Date of Birth\" Column: (Date only column)\n",
    "    a. Unified formate of the date\n",
    "    b. Generate a week date column \n",
    "2. For other specify Date Columns: (Here we assumne that all other Datetime \n",
    "       column has both datetime information)\n",
    "    a. Unify Date formate to long DateTime formate (\"MM/dd/yyyy hh:mm:ss a\")\n",
    "    b. Create the Date Column from DateTime column   \n",
    "    c. Create the Time Column from DateTime column drop the DateTime column\n",
    "    d. Generate Month column from Date column\n",
    "    e. Generate AM-PM column from Date column to indicate section of the date\n",
    "    f. Generate week date column\n",
    "    f. Drop origional/target DateTime column\n",
    "\n",
    "= input parameters:\n",
    "@ tgtDF      DataFrame  # This is should be existing path for the datafile\n",
    "@ colName    String     # This is name of the column in passed DataFrame need \n",
    "                          to be treated with\n",
    "@ sourceType String     # This is the string to defind in or out data source \n",
    "                          passed in string is imited to \"in\" or \"out\" only\n",
    "                          \"in\":  means data from inTake Data source file\n",
    "                          \"out\": means data from outCome Data source file\n",
    "\n",
    "* output parameters:\n",
    "@trim_df     DataFrame  # DataFrame that contain the extra column and dropped \n",
    "                          target column if not the date only column \n",
    "*/\n",
    "\n",
    "\n",
    "def Date_Time_Wrangler (tgtDF: DataFrame,colName: String,sourceType: String): DataFrame={\n",
    "\n",
    "//Section 1: Variable definition\n",
    "        val DBColName      = \"dob\"\n",
    "        var dateColName    = \"\"\n",
    "        var timeColName    = \"\"\n",
    "        var monthColName   = \"\"\n",
    "        var MorAftColName  = \"\"\n",
    "        lazy val DBweekDate = \"BirthWeekDate_out\"\n",
    "        var WeekColName    = \"\"\n",
    "\n",
    "//Section 2: UDF function declar\n",
    "        //Unify Full Format UDF variable definition\n",
    "        //Function check length of string to definded which format current it is. \n",
    "        //***Potential bug or exception could occur when input data formate change\n",
    "        var unifyTimeString = udf {\n",
    "            (dt: String) =>{\n",
    "                val  s24 = new SimpleDateFormat(\"M/d/yy HH:mm\")\n",
    "                val  d12 = new SimpleDateFormat(\"M/d/yy hh:mm:ss a\")\n",
    "                val  l24 = new SimpleDateFormat(\"MM/dd/yyyy HH:mm:ss\")\n",
    "                val  uniFormat = new SimpleDateFormat(\"MM/dd/yyyy hh:mm:ss a\")\n",
    "                //Remove DateTime string extra space\n",
    "                var cleanStr = dt.trim().replaceAll(\" +\", \" \")\n",
    "                \n",
    "                if (cleanStr.size >= 18){\n",
    "                   uniFormat.format(l24.parse(cleanStr))\n",
    "                }else{\n",
    "                    uniFormat.format(s24.parse(cleanStr))\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        //UDF of unify Date format\n",
    "        //Function check length of string to definded which format current it is. \n",
    "        //***Potential bug or exception could occur\n",
    "        val unifyDateFormat = udf{\n",
    "            (dt: String) =>{\n",
    "                val lformat = new SimpleDateFormat(\"MM/dd/yyyy\");\n",
    "                val sformat = new SimpleDateFormat(\"M/d/yy\");\n",
    "                if (dt.size == 10) {dt}\n",
    "                else {lformat.format(sformat.parse(dt))}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        //UDF of generate 3 charactors week date string\n",
    "        var getWeekDate = udf{\n",
    "            (dt: String) =>{\n",
    "            val shortWeek = new SimpleDateFormat(\"E\")\n",
    "            val lformat = new SimpleDateFormat(\"MM/dd/yyyy\")\n",
    "            shortWeek.format(lformat.parse(dt))\n",
    "            }\n",
    "        }\n",
    " \n",
    "        //UDF of generate 3 charactors week date string\n",
    "        var getMonth = udf{\n",
    "            (dt: String) =>{\n",
    "            val Month = new SimpleDateFormat(\"MMM\")\n",
    "            val lformat = new SimpleDateFormat(\"MM/dd/yyyy\")\n",
    "            Month.format(lformat.parse(dt))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "//Section 3: proceed DateTime data proceessing        \n",
    "    \n",
    "        colName match{\n",
    "            //If passed Date of Birth column\n",
    "            case DBColName => {\n",
    "                //unify Date formate\n",
    "                var format_df = tgtDF.withColumn(colName, unifyDateFormat(col(colName)))\n",
    "                //Generate 3 characters week date column.\n",
    "                var addWeek_df = format_df.withColumn(DBweekDate, getWeekDate(col(colName)))\n",
    "                return addWeek_df\n",
    "            }\n",
    "            //Processing other Date and Time Column\n",
    "            //It's consider Other datetime column that has both date and time\n",
    "            case _ => {\n",
    "                //Initial column name based on passed in source type\n",
    "                sourceType.toLowerCase match{\n",
    "                    //Initialized columns' name for in data source\n",
    "                    case \"in\" =>{\n",
    "                        WeekColName   = \"weekday_in\"\n",
    "                        dateColName   = \"date_in\"\n",
    "                        timeColName   = \"time_in\"\n",
    "                        monthColName  = \"month_in\"\n",
    "                        MorAftColName = \"ampm_in\"\n",
    "                    }\n",
    "                    //Initialized columns' name for out data source\n",
    "                    case \"out\"=>{\n",
    "                        WeekColName   = \"weekday_out\"\n",
    "                        dateColName   = \"date_out\"\n",
    "                        timeColName   = \"time_out\"\n",
    "                        monthColName  = \"month_out\"\n",
    "                        MorAftColName = \"ampm_out\"\n",
    "                    }\n",
    "                    //Other string will return origional Data Frame\n",
    "                    case _ => {\n",
    "                        println(\"Error in DateTimeWrangle: Please provid data source type as \\\"In\\\" or \\\"Out\\\"\")\n",
    "                        return tgtDF\n",
    "                    } \n",
    "                }\n",
    "                //Unified Date Time format\n",
    "                var trim_df = tgtDF.withColumn(colName, unifyTimeString(col(colName)))\n",
    "                //Generate Date column\n",
    "                trim_df = trim_df.withColumn(dateColName, split(col(colName), \"\\\\s\")\n",
    "                                 .getItem(0))\n",
    "                //Generate Time column and drop the target column\n",
    "                trim_df = trim_df.withColumn(timeColName, split(col(colName), \"\\\\s\")\n",
    "                                 .getItem(1))\n",
    "                //Generate AM or PM column\n",
    "                trim_df = trim_df.withColumn(MorAftColName, split(col(colName),\"\\\\s\")\n",
    "                                 .getItem(2)).drop(colName)\n",
    "                //Gerate 3 Characters Month Column\n",
    "                trim_df = trim_df.withColumn(monthColName, getMonth(col(dateColName)))\n",
    "                //Generate 3 characters week date column.\n",
    "                trim_df = trim_df.withColumn(WeekColName, getWeekDate(col(dateColName)))\n",
    "                return trim_df\n",
    "            }\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_nulls: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_nulls(df: DataFrame): DataFrame = {\n",
    "    val updated_df = df.na.fill(\"unknown\", Seq(\"name_in\"))\n",
    "                .na.fill(\"unknown\", Seq(\"location_found\"))\n",
    "                .na.fill(\"unknown\", Seq(\"intake_type\"))                                                                                   \n",
    "                .na.fill(\"unknown\", Seq(\"intake_condition\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"species_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"age_years_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"sex_only\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"desexed_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"prim_colour_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"prim_colour_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"sec_colour_in\"))                                                                                  \n",
    "                .na.fill(\"unknown\", Seq(\"sec_colour_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"prim_breed_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"sec_breed_in\"))                                                                                \n",
    "                .na.fill(\"unknown\", Seq(\"prim_breed_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"sec_breed_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"date_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"time_in\"))                                                                                \n",
    "                .na.fill(\"unknown\", Seq(\"time_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"ampm_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"month_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"id_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"weekday_in\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"outcome_type\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"outcome_subtype\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"age_years_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"desexed_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"date_out\"))                                                                                 \n",
    "                .na.fill(\"unknown\", Seq(\"month_out\"))                                                                                  \n",
    "                .na.fill(\"unknown\", Seq(\"ampm_out\"))                                                                                  \n",
    "                .na.fill(\"unknown\", Seq(\"weekday_out\"))\n",
    "    return updated_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_location_found: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_location_found(df: DataFrame): DataFrame = {\n",
    "\n",
    "    val pattern = \"\\\\w+ \\\\(\\\\w\\\\w\\\\)\".r\n",
    "    def extract_location (input:String) : String = {\n",
    "        if (input.matches(\"Outside Jurisdiction\")){\n",
    "            var town = input\n",
    "            return town\n",
    "        } else {\n",
    "            var town = pattern.findFirstIn(input).getOrElse(\"no match\")\n",
    "            return town\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Generate udf to apply to DataFrame column\n",
    "    val extractLocation = udf[String, String](extract_location)\n",
    "    \n",
    "    // Add new colour cols\n",
    "\n",
    "    var updated_df = df.withColumn(\"town_found\", extractLocation(scala.Symbol(\"location_found\")))\n",
    "    return updated_df   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_features: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Encoding categorical features to numeric values stored in vectors\n",
    "\n",
    "def encode_features(df: DataFrame): DataFrame = {\n",
    "    // Ensure only categorical columns are encoded\n",
    "    val feats = df.columns.filterNot(_.toLowerCase().contains(\"name\")).filterNot(_.toLowerCase().contains(\"date\")).filterNot(_.toLowerCase().contains(\"time\")).filterNot(_.toLowerCase().contains(\"dob\")).filterNot(_.toLowerCase().contains(\"age\")).filterNot(_.toLowerCase().contains(\"id\"))\n",
    "    // Define new encoded columns both indice and vectors\n",
    "    val encoded_feats = feats.flatMap{ name =>\n",
    "        val indexer = new StringIndexer()\n",
    "                        .setInputCol(name)\n",
    "                        .setOutputCol(name+\"_index\")\n",
    "        val encoder = new OneHotEncoderEstimator()\n",
    "                        .setInputCols(Array(name+\"_index\"))\n",
    "                        .setOutputCols(Array(name+\"_vector\"))\n",
    "                        .setDropLast(false)\n",
    "        Array(indexer, encoder)\n",
    "    }\n",
    "    val pipeline = new Pipeline().setStages(encoded_feats)  // Setting the encoding pipeline     \n",
    "    val df_transformed = pipeline.fit(df).transform(df) // Fitting the encoding pipeline to the dataframe\n",
    "    val non_index_cols = df_transformed.columns.filterNot(_.contains(\"_index\")).toList // List of column names that do not hold index\n",
    "    val vector_cols = df_transformed.columns.filter(_.contains(\"_vector\")).toSeq // Sequence of column names that hold vectors\n",
    "    var df_result = df_transformed.select(non_index_cols.head, non_index_cols.tail:_*) // Dataframe with all non-index columns\n",
    "    vector_cols.foreach(x => df_result = df_result.withColumn(s\"$x\", to_json(struct(s\"$x\"))))  // Convert columns that hold vectors to hold json of the vectors\n",
    "    return df_result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoUFlV4mHqD6"
   },
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCQbuPTUHqD6",
    "outputId": "476cbb7a-7bd8-40bc-b1dd-edf9a6f51847",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined schema\n",
      "root\n",
      " |-- id_in: string (nullable = true)\n",
      " |-- name_in: string (nullable = true)\n",
      " |-- location_found_in: string (nullable = true)\n",
      " |-- intake_type: string (nullable = true)\n",
      " |-- intake_condition: string (nullable = true)\n",
      " |-- species_in: string (nullable = true)\n",
      " |-- age_years_in: float (nullable = false)\n",
      " |-- sex_in: string (nullable = true)\n",
      " |-- desexed_in: string (nullable = true)\n",
      " |-- prim_colour_in: string (nullable = true)\n",
      " |-- sec_colour_in: string (nullable = true)\n",
      " |-- mix_breed_bool_in: string (nullable = true)\n",
      " |-- prim_breed_in: string (nullable = true)\n",
      " |-- town_found: string (nullable = true)\n",
      " |-- date_in: string (nullable = true)\n",
      " |-- time_in: string (nullable = true)\n",
      " |-- ampm_in: string (nullable = true)\n",
      " |-- month_in: string (nullable = true)\n",
      " |-- weekday_in: string (nullable = true)\n",
      " |-- dob_in: string (nullable = true)\n",
      " |-- outcome_type: string (nullable = true)\n",
      " |-- outcome_subtype: string (nullable = true)\n",
      " |-- age_years_out: float (nullable = false)\n",
      " |-- desexed_out: string (nullable = true)\n",
      " |-- date_out: string (nullable = true)\n",
      " |-- time_out: string (nullable = true)\n",
      " |-- ampm_out: string (nullable = true)\n",
      " |-- month_out: string (nullable = true)\n",
      " |-- weekday_out: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intake_file_path: String = Austin_Animal_Center_Intakes.csv\n",
       "outcomes_file_path: String = Austin_Animal_Center_Outcomes.csv\n",
       "in_df: org.apache.spark.sql.DataFrame = [animal_id: string, name_in: string ... 10 more fields]\n",
       "out_df: org.apache.spark.sql.DataFrame = [animal_id: string, name_out: string ... 10 more fields]\n",
       "in: org.apache.spark.sql.DataFrame = [animal_id_unique: string, name_in: string ... 21 more fields]\n",
       "out: org.apache.spark.sql.DataFrame = [animal_id_unique: string, name_out: string ... 20 more fields]\n",
       "in: org.apache.spark.sql.DataFrame = [animal_id_unique: string, name_in: string ... 21 more fields]\n",
       "out: org.apache.spark.sql.DataFrame = [animal_id_unique: string, name_out: string ... 20 more fields]\n",
       "in: org.apache.spark.sql.DataFrame = [animal_id_unique: string, name_in: str..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Change these to your local path\n",
    "val intake_file_path = \"Austin_Animal_Center_Intakes.csv\"\n",
    "val outcomes_file_path = \"Austin_Animal_Center_Outcomes.csv\"\n",
    "\n",
    "// Update import and dataframes\n",
    "var (in_df, out_df) = import_data_set_schema(intake_file_path, outcomes_file_path)\n",
    "var (in, out) = create_unique_id(in_df, out_df)\n",
    "in = convert_location_found(convert_breed(convert_colour(convert_sex(convert_age(in, \"in\"), \"in\"), \"in\"),\"in\"))\n",
    "out = convert_breed(convert_colour(convert_sex(convert_age(out, \"out\"), \"out\"), \"out\"), \"out\")\n",
    "in = Date_Time_Wrangler(in, \"datetime_in\", \"in\")\n",
    "out = Date_Time_Wrangler(out, \"datetime_out\", \"out\")\n",
    "out = Date_Time_Wrangler(out, \"dob\", \"out\")\n",
    "\n",
    "// Drop unnecessary columns\n",
    "var in_dropped = in.drop(\"sex_in\").drop(\"breed_in\").drop(\"color_in\").drop(\"age_in\")\n",
    "var out_dropped = out.drop(\"sex_out\").drop(\"sex_only\").drop(\"color_out\").drop(\"breed_out\").drop(\"age_out\").drop(\"species_out\").drop(\"prim_colour_out\").drop(\"sec_colour_out\").drop(\"prim_breed_out\").drop(\"BirthWeekDate_out\").drop(\"name_out\")\n",
    "\n",
    "// Join dataframes\n",
    "in_dropped = in_dropped.withColumnRenamed(\"animal_id_unique\", \"id_in\")\n",
    "out_dropped = out_dropped.withColumnRenamed(\"animal_id_unique\", \"id_out\")\n",
    "var joined = in_dropped.join(out_dropped).where($\"id_in\" === $\"id_out\").drop(\"id_out\")\n",
    "joined = joined.withColumnRenamed(\"sex_only\", \"sex_in\")\n",
    "joined = joined.withColumnRenamed(\"dob\", \"dob_in\")\n",
    "joined = joined.withColumnRenamed(\"location_found\", \"location_found_in\")\n",
    "joined = joined.withColumnRenamed(\"mix_bool\", \"mix_breed_bool_in\")\n",
    "\n",
    "//remove nulls\n",
    "val nulls_removed = convert_nulls(joined)\n",
    "\n",
    "//print schema\n",
    "println(\"joined schema\")\n",
    "joined.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-10 11:24:03,715 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " path file:/home/encoded_output already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/home/encoded_output already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  ... 39 elided",
      ""
     ]
    }
   ],
   "source": [
    "// Encode features\n",
    "val result = encode_features(nulls_removed)\n",
    "result.write.format(\"csv\").option(\"header\", \"true\").save(\"encoded_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " path file:/home/not_encoded_output already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/home/not_encoded_output already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)",
      "  ... 39 elided",
      ""
     ]
    }
   ],
   "source": [
    "// write un-encoded to csv\n",
    "nulls_removed.write.csv(\"not_encoded_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202_WranglePipeline.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
